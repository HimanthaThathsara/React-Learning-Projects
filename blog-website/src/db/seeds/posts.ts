import { db } from '@/db';
import { posts } from '@/db/schema';

async function main() {
    const samplePosts = [
        {
            title: 'Getting Started with Next.js 15',
            slug: 'getting-started-with-nextjs-15',
            content: 'Next.js 15 brings revolutionary changes to the React ecosystem with enhanced performance and developer experience. The latest version introduces improved server components, better caching strategies, and streamlined data fetching patterns that make building modern web applications more intuitive than ever.\n\nOne of the standout features is the enhanced App Router with improved streaming capabilities and partial prerendering. These features allow developers to build highly interactive applications while maintaining excellent performance characteristics. The new version also includes better TypeScript support and improved error handling mechanisms.\n\nServer Actions have been refined to provide more flexibility and better integration with form handling. The turbopack bundler, now stable, offers significantly faster build times compared to webpack, making the development experience more enjoyable. Additionally, the new metadata API simplifies SEO optimization with a more declarative approach.\n\nWhether you\'re building a simple blog or a complex e-commerce platform, Next.js 15 provides the tools and patterns needed to create production-ready applications. The framework continues to push the boundaries of what\'s possible with React while maintaining backwards compatibility and a smooth upgrade path.',
            excerpt: 'Discover the powerful new features in Next.js 15, including enhanced server components, improved caching, and the stable turbopack bundler for faster development.',
            featuredImage: 'https://images.unsplash.com/photo-1633356122544-f134324a6cee?w=800&h=400',
            categoryId: 2,
            authorId: 1,
            publishedAt: new Date('2024-11-15').toISOString(),
            views: 7850,
            readingTime: 8,
            createdAt: new Date('2024-11-14').toISOString(),
            updatedAt: new Date('2024-11-14').toISOString(),
        },
        {
            title: 'Understanding React Server Components',
            slug: 'understanding-react-server-components',
            content: 'React Server Components represent a paradigm shift in how we think about building React applications. Unlike traditional client-side components, Server Components run exclusively on the server, allowing developers to build interfaces that are both fast and feature-rich without sacrificing bundle size or user experience.\n\nThe key advantage of Server Components lies in their ability to access backend resources directly without requiring API routes. This means you can query databases, read files, and perform other server-side operations right within your component code. The result is cleaner, more maintainable code with fewer abstractions and better performance.\n\nServer Components also enable powerful composition patterns. You can mix Server and Client Components in the same component tree, allowing you to optimize which parts of your UI need interactivity and which can remain static. This hybrid approach gives you fine-grained control over your application\'s performance characteristics.\n\nUnderstanding when to use Server Components versus Client Components is crucial for building efficient React applications. Server Components are ideal for static content, data fetching, and accessing backend resources, while Client Components should be used for interactive elements that require state management or browser APIs.',
            excerpt: 'Learn how React Server Components revolutionize application architecture by enabling direct backend access and optimal performance through server-side rendering.',
            featuredImage: 'https://images.unsplash.com/photo-1633356122102-3fe601e05bd2?w=800&h=400',
            categoryId: 2,
            authorId: 1,
            publishedAt: new Date('2024-11-22').toISOString(),
            views: 6420,
            readingTime: 10,
            createdAt: new Date('2024-11-21').toISOString(),
            updatedAt: new Date('2024-11-21').toISOString(),
        },
        {
            title: 'Docker Best Practices for Production',
            slug: 'docker-best-practices-for-production',
            content: 'Running Docker containers in production requires careful consideration of security, performance, and reliability. Following best practices ensures your containerized applications run smoothly and securely in production environments. This guide covers essential patterns and techniques for production-ready Docker deployments.\n\nFirst and foremost, always use multi-stage builds to minimize image size and reduce attack surface. By separating build dependencies from runtime dependencies, you create leaner images that start faster and consume fewer resources. Additionally, use specific base image tags rather than \'latest\' to ensure reproducible builds and avoid unexpected breaking changes.\n\nSecurity should be a top priority when deploying containers to production. Run containers as non-root users, scan images for vulnerabilities regularly, and keep base images updated with security patches. Implement proper secrets management using tools like Docker secrets or external secret management systems rather than baking credentials into images.\n\nMonitoring and logging are critical for production Docker deployments. Implement centralized logging with tools like ELK stack or Loki, and use health checks to ensure containers are functioning correctly. Resource limits should be set for all containers to prevent resource exhaustion and ensure fair resource allocation across your cluster.',
            excerpt: 'Master essential Docker best practices for production deployments, including multi-stage builds, security hardening, and effective monitoring strategies.',
            featuredImage: 'https://images.unsplash.com/photo-1605745341112-85968b19335b?w=800&h=400',
            categoryId: 4,
            authorId: 2,
            publishedAt: new Date('2024-11-08').toISOString(),
            views: 8250,
            readingTime: 12,
            createdAt: new Date('2024-11-07').toISOString(),
            updatedAt: new Date('2024-11-07').toISOString(),
        },
        {
            title: 'Introduction to TypeScript: A Comprehensive Guide',
            slug: 'introduction-to-typescript-comprehensive-guide',
            content: 'TypeScript has become the de facto standard for building large-scale JavaScript applications. By adding static typing to JavaScript, TypeScript helps catch errors early in the development process and provides better tooling support through intelligent code completion and refactoring capabilities.\n\nThe type system in TypeScript is both powerful and flexible. You can start with basic types like string, number, and boolean, then progress to more advanced concepts like generics, union types, and conditional types. The gradual typing approach means you can adopt TypeScript incrementally, adding types as your application grows and evolves.\n\nOne of TypeScript\'s greatest strengths is its integration with modern development tools. IDEs like Visual Studio Code provide exceptional TypeScript support with features like inline error detection, automatic imports, and intelligent code suggestions. The compiler\'s strict mode catches common mistakes and enforces best practices, leading to more maintainable codebases.\n\nTypeScript also excels at defining and documenting APIs through interfaces and type definitions. These serve as living documentation that stays in sync with your code, making it easier for teams to collaborate and understand complex systems. The ecosystem of DefinitelyTyped provides type definitions for thousands of JavaScript libraries, enabling type safety across your entire stack.',
            excerpt: 'Explore the fundamentals of TypeScript and discover how static typing can transform your JavaScript development workflow with better tooling and error prevention.',
            featuredImage: 'https://images.unsplash.com/photo-1516116216624-53e697fedbea?w=800&h=400',
            categoryId: 2,
            authorId: 1,
            publishedAt: new Date('2024-10-28').toISOString(),
            views: 5890,
            readingTime: 11,
            createdAt: new Date('2024-10-27').toISOString(),
            updatedAt: new Date('2024-10-27').toISOString(),
        },
        {
            title: 'Building REST APIs with Node.js and Express',
            slug: 'building-rest-apis-nodejs-express',
            content: 'Building robust REST APIs is a fundamental skill for modern backend development. Node.js and Express provide a powerful combination for creating scalable, maintainable API services that power web and mobile applications. This guide walks through best practices and essential patterns for building production-ready REST APIs.\n\nStart by structuring your application with clear separation of concerns. Organize your code into routes, controllers, services, and models to maintain clean architecture. Use middleware effectively for cross-cutting concerns like authentication, logging, and error handling. Express middleware chain provides elegant composition for request processing.\n\nImplement proper error handling throughout your API. Create custom error classes for different error types and use a centralized error handling middleware to format error responses consistently. Always validate input data using libraries like Joi or express-validator to prevent invalid data from reaching your business logic.\n\nSecurity is paramount for API development. Implement rate limiting to prevent abuse, use helmet for security headers, and always validate and sanitize user input. Use JWT tokens for stateless authentication and implement proper CORS policies. Remember to log all API activity for debugging and audit purposes while being mindful of sensitive data.',
            excerpt: 'Learn to build scalable REST APIs using Node.js and Express with proper architecture, security measures, and best practices for production environments.',
            featuredImage: 'https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=400',
            categoryId: 2,
            authorId: 4,
            publishedAt: new Date('2024-11-05').toISOString(),
            views: 7120,
            readingTime: 9,
            createdAt: new Date('2024-11-04').toISOString(),
            updatedAt: new Date('2024-11-04').toISOString(),
        },
        {
            title: 'Cloud Architecture Patterns Explained',
            slug: 'cloud-architecture-patterns-explained',
            content: 'Understanding cloud architecture patterns is essential for building scalable, resilient applications in modern cloud environments. These proven patterns help solve common challenges in distributed systems while leveraging the unique capabilities of cloud platforms like AWS, Azure, and Google Cloud.\n\nThe microservices pattern enables building applications as a collection of loosely coupled services. Each service is independently deployable and scalable, allowing teams to work autonomously and deploy updates without affecting the entire system. Service mesh technologies like Istio provide advanced traffic management, security, and observability for microservices architectures.\n\nEvent-driven architecture is another powerful pattern for cloud applications. By using message queues and event streams, services can communicate asynchronously, leading to better decoupling and resilience. Services react to events rather than making direct calls, which improves scalability and allows for easier system evolution.\n\nThe strangler fig pattern is particularly useful for migrating legacy applications to the cloud. Instead of a risky big-bang migration, you gradually replace parts of the old system with new cloud-native components. This approach minimizes risk while allowing you to benefit from cloud capabilities incrementally.',
            excerpt: 'Discover essential cloud architecture patterns including microservices, event-driven architecture, and strangler fig for building resilient distributed systems.',
            featuredImage: 'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&h=400',
            categoryId: 6,
            authorId: 2,
            publishedAt: new Date('2024-10-18').toISOString(),
            views: 6740,
            readingTime: 13,
            createdAt: new Date('2024-10-17').toISOString(),
            updatedAt: new Date('2024-10-17').toISOString(),
        },
        {
            title: 'Kubernetes Fundamentals for Beginners',
            slug: 'kubernetes-fundamentals-for-beginners',
            content: 'Kubernetes has become the standard platform for container orchestration, but its complexity can be overwhelming for beginners. This guide breaks down the fundamental concepts and helps you understand how Kubernetes manages containerized applications at scale.\n\nAt its core, Kubernetes provides a declarative way to manage containers. You describe the desired state of your application in YAML files, and Kubernetes continuously works to maintain that state. The key building blocks include Pods (the smallest deployable units), Deployments (manage replica sets), Services (enable networking), and ConfigMaps/Secrets (handle configuration).\n\nUnderstanding the control plane is crucial for working with Kubernetes. The API server acts as the front-end, the scheduler assigns pods to nodes, and the controller manager ensures the actual state matches the desired state. The etcd key-value store holds all cluster data, making it critical for cluster operation.\n\nWorking with kubectl, the command-line tool for Kubernetes, becomes second nature with practice. Learn the common commands for creating, inspecting, and debugging resources. Use namespaces to organize resources, and leverage labels and selectors for flexible resource management. Start with a local cluster using minikube or kind before moving to production environments.',
            excerpt: 'Master Kubernetes basics with this beginner-friendly guide covering pods, deployments, services, and the control plane architecture.',
            featuredImage: 'https://images.unsplash.com/photo-1667372393119-3d4c48d07fc9?w=800&h=400',
            categoryId: 4,
            authorId: 2,
            publishedAt: new Date('2024-11-12').toISOString(),
            views: 8520,
            readingTime: 14,
            createdAt: new Date('2024-11-11').toISOString(),
            updatedAt: new Date('2024-11-11').toISOString(),
        },
        {
            title: 'Machine Learning Model Deployment Strategies',
            slug: 'machine-learning-model-deployment-strategies',
            content: 'Deploying machine learning models to production presents unique challenges that go beyond traditional software deployment. Understanding various deployment strategies and their trade-offs is crucial for building reliable ML-powered applications that deliver value to users.\n\nThe simplest deployment approach is batch prediction, where models process large datasets periodically. This works well for use cases that don\'t require real-time predictions, such as recommendation systems that update daily. Batch processing allows for optimized resource utilization and easier monitoring of model performance over time.\n\nFor real-time predictions, you need an online serving infrastructure. Options include REST APIs using frameworks like FastAPI or Flask, which provide flexibility but require careful attention to latency and scaling. Alternatively, gRPC offers better performance for microservices architectures, while specialized serving platforms like TensorFlow Serving provide optimized inference.\n\nModel monitoring and versioning are critical for production ML systems. Implement A/B testing to compare model versions, track data drift to detect when model retraining is needed, and monitor prediction latency and throughput. Use feature stores to ensure consistency between training and serving, and implement shadow mode deployment to validate new models before fully rolling them out.',
            excerpt: 'Explore effective strategies for deploying machine learning models to production, including batch processing, real-time serving, and monitoring approaches.',
            featuredImage: 'https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&h=400',
            categoryId: 3,
            authorId: 3,
            publishedAt: new Date('2024-10-25').toISOString(),
            views: 5640,
            readingTime: 11,
            createdAt: new Date('2024-10-24').toISOString(),
            updatedAt: new Date('2024-10-24').toISOString(),
        },
        {
            title: 'Neural Networks: From Theory to Practice',
            slug: 'neural-networks-from-theory-to-practice',
            content: 'Neural networks form the foundation of modern deep learning, powering everything from image recognition to natural language processing. Understanding how they work from first principles helps you build better models and debug issues when they arise. This guide bridges the gap between theory and practical implementation.\n\nAt their core, neural networks are composed of layers of interconnected nodes that transform input data through learned weights and activation functions. The forward pass computes predictions by multiplying inputs by weights and applying non-linear activation functions. The backward pass uses gradient descent to adjust weights based on the difference between predictions and actual values.\n\nChoosing the right architecture is crucial for success. Convolutional neural networks excel at computer vision tasks by learning spatial hierarchies of features. Recurrent networks and transformers handle sequential data like text and time series. Understanding when to use each architecture and how to configure hyperparameters comes with experience and experimentation.\n\nPractical considerations matter as much as theory. Use techniques like batch normalization to stabilize training, dropout to prevent overfitting, and learning rate scheduling to converge faster. Start with pre-trained models when possible through transfer learning, which dramatically reduces training time and data requirements. Always validate your model on held-out test data to ensure it generalizes well.',
            excerpt: 'Dive deep into neural networks, covering fundamental concepts, architectural choices, and practical techniques for building effective deep learning models.',
            featuredImage: 'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&h=400',
            categoryId: 3,
            authorId: 3,
            publishedAt: new Date('2024-11-18').toISOString(),
            views: 4920,
            readingTime: 15,
            createdAt: new Date('2024-11-17').toISOString(),
            updatedAt: new Date('2024-11-17').toISOString(),
        },
        {
            title: 'React Native vs Flutter: A Detailed Comparison',
            slug: 'react-native-vs-flutter-detailed-comparison',
            content: 'Choosing between React Native and Flutter for mobile app development is a crucial decision that impacts your project\'s success. Both frameworks enable cross-platform development, but they take different approaches and excel in different scenarios. This comprehensive comparison helps you make an informed choice.\n\nReact Native leverages JavaScript and React, making it accessible to web developers. It renders native components, providing an authentic platform-specific feel. The large ecosystem of npm packages and strong community support are major advantages. However, bridging between JavaScript and native code can introduce performance bottlenecks in complex applications.\n\nFlutter uses Dart and renders everything on a custom canvas, ensuring pixel-perfect consistency across platforms. The hot reload feature and comprehensive widget library make development incredibly fast. Flutter\'s performance is generally excellent because it compiles to native code and doesn\'t require a JavaScript bridge. The downside is a smaller ecosystem and learning curve for Dart.\n\nConsider React Native if you have existing React expertise, need access to specific native modules, or want a larger pool of JavaScript developers. Choose Flutter for apps requiring custom UI designs, high-performance animations, or when you prefer a more batteries-included framework with strong tooling support.',
            excerpt: 'Compare React Native and Flutter across performance, developer experience, ecosystem, and use cases to choose the right framework for your mobile app.',
            featuredImage: 'https://images.unsplash.com/photo-1512941937669-90a1b58e7e9c?w=800&h=400',
            categoryId: 5,
            authorId: 4,
            publishedAt: new Date('2024-10-22').toISOString(),
            views: 7680,
            readingTime: 10,
            createdAt: new Date('2024-10-21').toISOString(),
            updatedAt: new Date('2024-10-21').toISOString(),
        },
        {
            title: 'Securing Your Web Applications: Top 10 Tips',
            slug: 'securing-web-applications-top-10-tips',
            content: 'Web application security should be a priority from day one, not an afterthought. With cyber attacks becoming increasingly sophisticated, implementing robust security measures is essential for protecting user data and maintaining trust. These ten tips provide a solid foundation for securing your web applications.\n\nFirst, always validate and sanitize user input on both client and server side. SQL injection and cross-site scripting (XSS) attacks exploit insufficient input validation. Use parameterized queries for database operations and escape output appropriately. Content Security Policy headers provide an additional layer of defense against XSS attacks.\n\nImplement proper authentication and authorization. Use strong password policies, enable multi-factor authentication, and never store passwords in plain text—always use bcrypt or similar algorithms. Implement role-based access control to ensure users can only access resources they\'re authorized for. Session management should use secure, HTTP-only cookies with appropriate expiration times.\n\nKeep dependencies updated and monitor for vulnerabilities. Tools like npm audit and Snyk help identify known vulnerabilities in third-party packages. Enable HTTPS everywhere with properly configured TLS certificates. Use security headers like HSTS, X-Frame-Options, and X-Content-Type-Options. Regular security audits and penetration testing help identify vulnerabilities before attackers do.',
            excerpt: 'Implement essential security measures for web applications, from input validation and authentication to dependency management and security headers.',
            featuredImage: 'https://images.unsplash.com/photo-1555949963-ff9fe0c870eb?w=800&h=400',
            categoryId: 7,
            authorId: 5,
            publishedAt: new Date('2024-11-01').toISOString(),
            views: 6330,
            readingTime: 12,
            createdAt: new Date('2024-10-31').toISOString(),
            updatedAt: new Date('2024-10-31').toISOString(),
        },
        {
            title: 'OAuth 2.0 and JWT Authentication Explained',
            slug: 'oauth-2-jwt-authentication-explained',
            content: 'Modern web applications require robust authentication mechanisms that provide security without sacrificing user experience. OAuth 2.0 and JSON Web Tokens (JWT) have become industry standards for implementing authentication and authorization. Understanding how they work together helps you build secure, scalable authentication systems.\n\nOAuth 2.0 is an authorization framework that enables applications to obtain limited access to user accounts. It works by delegating user authentication to the service hosting the user account and authorizing third-party applications to access that account. The framework defines four roles: resource owner, client, authorization server, and resource server.\n\nJWT is a compact, URL-safe token format for transmitting information between parties. A JWT contains three parts: header, payload, and signature. The payload can include user identity and permissions, while the signature ensures the token hasn\'t been tampered with. JWTs are stateless, meaning servers don\'t need to store session information, which aids scalability.\n\nImplementing OAuth 2.0 with JWT requires careful attention to security. Always validate tokens properly, use short expiration times with refresh tokens, store tokens securely, and implement token revocation. Use the authorization code flow with PKCE for public clients and ensure your authorization server implements rate limiting and proper error handling.',
            excerpt: 'Learn how OAuth 2.0 and JWT work together to provide secure authentication and authorization for modern web applications.',
            featuredImage: 'https://images.unsplash.com/photo-1614064641938-3bbee52942c7?w=800&h=400',
            categoryId: 7,
            authorId: 5,
            publishedAt: new Date('2024-11-25').toISOString(),
            views: 5240,
            readingTime: 13,
            createdAt: new Date('2024-11-24').toISOString(),
            updatedAt: new Date('2024-11-24').toISOString(),
        },
        {
            title: 'Serverless Architecture with AWS Lambda',
            slug: 'serverless-architecture-aws-lambda',
            content: 'Serverless computing represents a paradigm shift in how we build and deploy applications. AWS Lambda, the pioneer in Function-as-a-Service (FaaS), allows you to run code without provisioning or managing servers. You pay only for compute time consumed, making it cost-effective for many use cases.\n\nAWS Lambda functions respond to events from various AWS services. API Gateway can trigger functions for HTTP requests, S3 events for file uploads, DynamoDB streams for database changes, and more. This event-driven model enables building highly scalable applications with minimal operational overhead. Functions automatically scale based on incoming requests.\n\nDesigning serverless applications requires thinking differently about architecture. Break applications into small, single-purpose functions that do one thing well. Use environment variables for configuration, implement proper error handling with dead letter queues, and leverage Lambda layers for sharing code and dependencies across functions.\n\nCold starts are a consideration for Lambda functions, especially for infrequently used functions or those with large dependencies. Strategies to mitigate cold starts include keeping functions warm with scheduled invocations, optimizing package size, and using provisioned concurrency for critical functions. Monitor function metrics with CloudWatch to optimize performance and costs.',
            excerpt: 'Discover how to build scalable, cost-effective applications using AWS Lambda and serverless architecture patterns.',
            featuredImage: 'https://images.unsplash.com/photo-1523474253046-8cd2748b5fd2?w=800&h=400',
            categoryId: 6,
            authorId: 2,
            publishedAt: new Date('2024-10-15').toISOString(),
            views: 7450,
            readingTime: 11,
            createdAt: new Date('2024-10-14').toISOString(),
            updatedAt: new Date('2024-10-14').toISOString(),
        },
        {
            title: 'Contributing to Open Source: A Beginner\'s Guide',
            slug: 'contributing-to-open-source-beginners-guide',
            content: 'Contributing to open source projects is an excellent way to improve your skills, build your portfolio, and give back to the community. However, making your first contribution can feel intimidating. This guide walks you through the process and provides tips for becoming a successful open source contributor.\n\nStart by finding projects that align with your interests and skill level. Look for repositories with good documentation, active maintainers, and issues labeled "good first issue" or "beginner friendly." Read the project\'s contributing guidelines carefully—they often contain valuable information about code style, testing requirements, and the contribution process.\n\nBefore writing code, engage with the community. Join their Discord or Slack, introduce yourself, and ask questions. Comment on issues to understand the problem better and propose solutions. Many maintainers appreciate when contributors discuss approaches before submitting pull requests, especially for significant changes.\n\nWhen submitting a pull request, follow the project\'s conventions for commit messages and code formatting. Write clear descriptions explaining what your changes do and why they\'re needed. Be responsive to feedback and don\'t take criticism personally—code review is about improving the code, not attacking you. Remember that maintainers are volunteers too, so be patient and respectful.',
            excerpt: 'Learn how to make your first open source contribution with practical advice on finding projects, engaging with communities, and submitting quality pull requests.',
            featuredImage: 'https://images.unsplash.com/photo-1556761175-b413da4baf72?w=800&h=400',
            categoryId: 8,
            authorId: 1,
            publishedAt: new Date('2024-10-08').toISOString(),
            views: 4560,
            readingTime: 9,
            createdAt: new Date('2024-10-07').toISOString(),
            updatedAt: new Date('2024-10-07').toISOString(),
        },
        {
            title: 'GraphQL vs REST: Which Should You Choose?',
            slug: 'graphql-vs-rest-which-should-you-choose',
            content: 'The debate between GraphQL and REST continues to evolve as both technologies mature and find their ideal use cases. Understanding the strengths and limitations of each helps you make informed architectural decisions for your API design.\n\nREST APIs have been the standard for years, offering simplicity and wide adoption. They use standard HTTP methods and status codes, making them easy to understand and cache. RESTful endpoints are predictable, and the stateless nature scales well. However, REST can lead to over-fetching or under-fetching data, requiring multiple round trips or returning excessive data.\n\nGraphQL provides a query language that allows clients to request exactly the data they need. A single endpoint serves all queries, and the strongly-typed schema provides excellent documentation and tooling support. Real-time subscriptions and efficient data fetching make GraphQL attractive for complex, data-rich applications. The learning curve is steeper, and caching requires more sophisticated strategies.\n\nChoose REST for public APIs where simplicity and caching are priorities, when you need broad client compatibility, or for simple CRUD operations. Pick GraphQL for complex domains with many interrelated entities, when mobile clients need to minimize data transfer, or when you want to provide a flexible API for diverse client needs. Many organizations successfully use both, applying each where it fits best.',
            excerpt: 'Compare GraphQL and REST APIs across flexibility, performance, and complexity to determine which approach best fits your application needs.',
            featuredImage: 'https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400',
            categoryId: 2,
            authorId: 4,
            publishedAt: new Date('2024-11-20').toISOString(),
            views: 6890,
            readingTime: 10,
            createdAt: new Date('2024-11-19').toISOString(),
            updatedAt: new Date('2024-11-19').toISOString(),
        },
        {
            title: 'Python for Data Science: Essential Libraries',
            slug: 'python-for-data-science-essential-libraries',
            content: 'Python has become the lingua franca of data science, thanks to its extensive ecosystem of libraries designed for data manipulation, analysis, and machine learning. Understanding the core libraries and their roles is essential for anyone working in data science.\n\nNumPy provides the foundation for numerical computing in Python. Its ndarray data structure enables efficient operations on large datasets, and its broadcasting capabilities simplify complex mathematical operations. NumPy\'s vectorized operations are significantly faster than pure Python loops, making it indispensable for performance-critical numerical computations.\n\nPandas builds on NumPy to provide high-level data structures and tools for data analysis. The DataFrame object is particularly powerful for working with tabular data, offering intuitive methods for filtering, grouping, and transforming data. Pandas excels at handling missing data, time series, and integrating data from various sources.\n\nFor machine learning, scikit-learn offers a consistent interface for a wide range of algorithms. From preprocessing and feature engineering to model training and evaluation, scikit-learn provides battle-tested implementations. Matplotlib and Seaborn handle visualization, helping you explore data and communicate findings effectively. For deep learning, TensorFlow and PyTorch provide powerful frameworks with GPU acceleration.',
            excerpt: 'Master essential Python libraries for data science including NumPy, Pandas, and scikit-learn to efficiently analyze data and build machine learning models.',
            featuredImage: 'https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400',
            categoryId: 3,
            authorId: 3,
            publishedAt: new Date('2024-10-12').toISOString(),
            views: 5180,
            readingTime: 12,
            createdAt: new Date('2024-10-11').toISOString(),
            updatedAt: new Date('2024-10-11').toISOString(),
        },
        {
            title: 'CI/CD Pipeline Best Practices',
            slug: 'cicd-pipeline-best-practices',
            content: 'Continuous Integration and Continuous Deployment pipelines are essential for modern software development, enabling teams to deliver features rapidly while maintaining quality. Implementing CI/CD effectively requires understanding best practices and avoiding common pitfalls.\n\nStart with a solid version control strategy. Use feature branches for development and protect your main branch with required status checks. Implement automated testing at multiple levels: unit tests for individual components, integration tests for system interactions, and end-to-end tests for critical user flows. Fast feedback is crucial—optimize your test suite to run quickly while maintaining comprehensive coverage.\n\nBuild artifacts once and promote them through environments rather than rebuilding for each stage. This ensures consistency and reduces deployment time. Use infrastructure as code tools like Terraform or CloudFormation to manage environments declaratively. Implement blue-green or canary deployments to minimize risk when releasing changes to production.\n\nMonitoring and observability should be built into your pipeline. Track deployment metrics, automate rollbacks when issues are detected, and maintain audit logs of all changes. Use feature flags to decouple deployment from release, allowing you to test features in production before making them available to users. Regular pipeline maintenance and optimization keep your CI/CD system effective as your application grows.',
            excerpt: 'Build robust CI/CD pipelines with best practices for testing, deployment strategies, and monitoring to accelerate software delivery safely.',
            featuredImage: 'https://images.unsplash.com/photo-1618401471353-b98afee0b2eb?w=800&h=400',
            categoryId: 4,
            authorId: 2,
            publishedAt: new Date('2024-10-30').toISOString(),
            views: 7920,
            readingTime: 11,
            createdAt: new Date('2024-10-29').toISOString(),
            updatedAt: new Date('2024-10-29').toISOString(),
        },
        {
            title: 'Building Progressive Web Apps (PWA)',
            slug: 'building-progressive-web-apps-pwa',
            content: 'Progressive Web Apps combine the best of web and mobile applications, providing app-like experiences that work reliably regardless of network conditions. PWAs have become increasingly popular as they offer native-like functionality without requiring app store distribution.\n\nThe foundation of a PWA is the service worker, a script that runs in the background and intercepts network requests. Service workers enable offline functionality by caching assets and API responses, allowing your app to work even without an internet connection. The Cache API provides fine-grained control over what to cache and how to serve cached content.\n\nA web app manifest defines how your PWA appears when installed on a device. Configure the app name, icons, theme colors, and display mode to create a native-like experience. Users can add PWAs to their home screen, and the app launches in standalone mode without browser chrome, making it feel like a native application.\n\nImplement best practices for PWA development: use HTTPS for security, design for mobile-first experiences, optimize for performance with lazy loading and code splitting, and provide meaningful offline experiences. Test across different network conditions and devices. Use Lighthouse audits to ensure your PWA meets quality standards and provides the best possible user experience.',
            excerpt: 'Create Progressive Web Apps that deliver native-like experiences with offline support, push notifications, and installability using modern web technologies.',
            featuredImage: 'https://images.unsplash.com/photo-1551650975-87deedd944c3?w=800&h=400',
            categoryId: 2,
            authorId: 1,
            publishedAt: new Date('2024-11-28').toISOString(),
            views: 4750,
            readingTime: 10,
            createdAt: new Date('2024-11-27').toISOString(),
            updatedAt: new Date('2024-11-27').toISOString(),
        },
    ];

    await db.insert(posts).values(samplePosts);
    
    console.log('✅ Posts seeder completed successfully');
}

main().catch((error) => {
    console.error('❌ Seeder failed:', error);
});